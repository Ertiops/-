{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load('ru_core_news_lg')\n",
    "\n",
    "with open('термины биологии', 'rb') as f: # загрузка словаря\n",
    "    data_new = pickle.load(f)\n",
    "\n",
    "lemmatized_terms_61k = data_new.keys()\n",
    "\n",
    "document = 'Р_Инф_2(2)_2_МаЧе_0_А_0_2012_5902' # название анализируемого  текста\n",
    "lines = [i.strip() for i in open(f'{document}.txt', 'r', encoding='utf-8').readlines()]\n",
    "stop_symbols = '''.:;–?\"-«»#_—!=)(!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~'''\n",
    "\n",
    "terms = [] # идентифицированные термины\n",
    "ignoring_for_text = [] # для хранения информации о токенах, которые определены как термины\n",
    "ignoring_for_symbols = [] # для запрета обращения к знакам препинания\n",
    "all_words = [] # для подсчета всех слов\n",
    "terms_tokens = [] # для подсчета словоформ-терминов\n",
    "\n",
    "\n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    indexes_to_ignore = []\n",
    "    symbols_to_ignore = []\n",
    "    terms_in_paragraph = []\n",
    "    for i in range(len(doc)):\n",
    "        collocation_candidate = []\n",
    "        if i <= len(doc) - 6 and doc[i].text not in stop_symbols: \n",
    "            collocation_candidate.append(doc[i].lemma_)\n",
    "            collocation_candidate.append(doc[i+1].lemma_)\n",
    "            collocation_candidate.append(doc[i+2].lemma_)\n",
    "            collocation_candidate.append(doc[i+3].lemma_)\n",
    "            collocation_candidate.append(doc[i+4].lemma_)\n",
    "            collocation_candidate.append(doc[i+5].lemma_)\n",
    "            lemmatized_collocation_candidate = \" \".join(collocation_candidate)\n",
    "            if lemmatized_collocation_candidate in lemmatized_terms_61k:\n",
    "                terms_in_paragraph.append(lemmatized_collocation_candidate)\n",
    "                indexes_to_ignore.append(i)\n",
    "                indexes_to_ignore.append(i + 1)\n",
    "                indexes_to_ignore.append(i + 2)\n",
    "                indexes_to_ignore.append(i + 3)\n",
    "                indexes_to_ignore.append(i + 4)\n",
    "                indexes_to_ignore.append(i + 5)\n",
    "        elif i <= len(doc) - 6 and doc[i].text in stop_symbols:\n",
    "            symbols_to_ignore.append(i)\n",
    "    ignoring_for_text.append(indexes_to_ignore)\n",
    "    ignoring_for_symbols.append(symbols_to_ignore)  \n",
    "    terms.append(terms_in_paragraph)\n",
    "\n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        if i <= len(doc) - 5 and i not in ignoring_for_text[line] and doc[i].text not in stop_symbols:\n",
    "            collocation_candidate.append(doc[i].lemma_)\n",
    "            collocation_candidate.append(doc[i+1].lemma_)\n",
    "            collocation_candidate.append(doc[i+2].lemma_)\n",
    "            collocation_candidate.append(doc[i+3].lemma_)\n",
    "            collocation_candidate.append(doc[i+4].lemma_)\n",
    "            lemmatized_collocation_candidate = \" \".join(collocation_candidate)\n",
    "            if lemmatized_collocation_candidate in lemmatized_terms_61k:\n",
    "                terms[line].append(lemmatized_collocation_candidate)\n",
    "                ignoring_for_text[line].append(i)\n",
    "                ignoring_for_text[line].append(i + 1)\n",
    "                ignoring_for_text[line].append(i + 2)\n",
    "                ignoring_for_text[line].append(i + 3)\n",
    "                ignoring_for_text[line].append(i + 4)\n",
    "        elif i <= len(doc) - 5 and doc[i].text in stop_symbols:\n",
    "            if i not in ignoring_for_symbols[line]:\n",
    "                ignoring_for_symbols[line].append(i)  \n",
    "\n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        collocation_candidate = []\n",
    "        if i <= len(doc) - 4 and i not in ignoring_for_text[line] and doc[i].text not in stop_symbols:\n",
    "            collocation_candidate.append(doc[i].lemma_)\n",
    "            collocation_candidate.append(doc[i+1].lemma_)\n",
    "            collocation_candidate.append(doc[i+2].lemma_)\n",
    "            collocation_candidate.append(doc[i+3].lemma_)\n",
    "            lemmatized_collocation_candidate = \" \".join(collocation_candidate)\n",
    "            if lemmatized_collocation_candidate in lemmatized_terms_61k:\n",
    "                terms[line].append(lemmatized_collocation_candidate)\n",
    "                ignoring_for_text[line].append(i)\n",
    "                ignoring_for_text[line].append(i + 1)\n",
    "                ignoring_for_text[line].append(i + 2)\n",
    "                ignoring_for_text[line].append(i + 3)\n",
    "        elif i <= len(doc) - 4 and doc[i].text in stop_symbols:\n",
    "            if i not in ignoring_for_symbols[line]:\n",
    "                ignoring_for_symbols[line].append(i)\n",
    "        \n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        collocation_candidate = []\n",
    "        if i <= len(doc) - 3 and i not in ignoring_for_text[line] and doc[i].text not in stop_symbols: \n",
    "            collocation_candidate.append(doc[i].lemma_)\n",
    "            collocation_candidate.append(doc[i+1].lemma_)\n",
    "            collocation_candidate.append(doc[i+2].lemma_)\n",
    "            lemmatized_collocation_candidate = \" \".join(collocation_candidate)\n",
    "            if lemmatized_collocation_candidate in lemmatized_terms_61k:\n",
    "                terms[line].append(lemmatized_collocation_candidate)\n",
    "                ignoring_for_text[line].append(i)\n",
    "                ignoring_for_text[line].append(i + 1)\n",
    "                ignoring_for_text[line].append(i + 2)\n",
    "        elif i <= len(doc) - 3  and doc[i].text in stop_symbols:\n",
    "            if i not in ignoring_for_symbols[line]:\n",
    "                ignoring_for_symbols[line].append(i)\n",
    "                \n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        collocation_candidate = []\n",
    "        if i <= len(doc) - 2 and i not in ignoring_for_text[line] and doc[i].text not in stop_symbols:\n",
    "            collocation_candidate.append(doc[i].lemma_)\n",
    "            collocation_candidate.append(doc[i+1].lemma_)\n",
    "            lemmatized_collocation_candidate = \" \".join(collocation_candidate)\n",
    "            if lemmatized_collocation_candidate in lemmatized_terms_61k:\n",
    "                terms[line].append(lemmatized_collocation_candidate)\n",
    "                ignoring_for_text[line].append(i)\n",
    "                ignoring_for_text[line].append(i + 1)\n",
    "        elif i <= len(doc) - 2 and doc[i].text in stop_symbols:\n",
    "            if i not in ignoring_for_symbols[line]:\n",
    "                ignoring_for_symbols[line].append(i)\n",
    "\n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        if i not in ignoring_for_text[line] and doc[i].text not in stop_symbols: \n",
    "            lemmatized_term_candidate = doc[i].lemma_\n",
    "            if lemmatized_term_candidate in lemmatized_terms_61k:\n",
    "                terms[line].append(lemmatized_term_candidate)\n",
    "                ignoring_for_text[line].append(i)\n",
    "        elif doc[i].text in stop_symbols:\n",
    "            if i not in ignoring_for_symbols[line]:\n",
    "                ignoring_for_symbols[line].append(i)\n",
    "                \n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text not in stop_symbols:\n",
    "            all_words.append(doc[i])  \n",
    "        if i in ignoring_for_text[line]:\n",
    "            terms_tokens.append(doc[i])\n",
    "\n",
    "text_terms = []\n",
    "\n",
    "for i in terms:\n",
    "    for j in i:\n",
    "        text_terms.append(data_new[j])\n",
    "        \n",
    "unique_terms = {}\n",
    "\n",
    "for i in text_terms:\n",
    "    if i not in unique_terms:\n",
    "        unique_terms[i] = 1\n",
    "    else:\n",
    "        unique_terms[i] += 1\n",
    "unique_terms = {k: unique_terms[k] for k in sorted(unique_terms, key=unique_terms.get, reverse=True)}\n",
    "            \n",
    "quantity_words = len(all_words)\n",
    "quantity_terms = len(terms_tokens)\n",
    "terms_percentage = round((quantity_terms / quantity_words), 3)\n",
    "unique_terms_quantity = len(unique_terms)\n",
    "\n",
    "fout=open(f'{document}.html', 'w', encoding='utf-8')\n",
    "\n",
    "fout.write('<html>'+'\\n')\n",
    "fout.write('<body>'+'\\n')\n",
    "        \n",
    "# блок визуализации\n",
    "\n",
    "for line in range(len(lines)):\n",
    "    doc = nlp(lines[line])\n",
    "    line_html = ''\n",
    "    for i in range(len(doc)):\n",
    "        if i in ignoring_for_text[line]:\n",
    "            line_html = line_html + '<span style=\"color:red\">' + doc[i].text +'</span>' + ' '\n",
    "        else:\n",
    "            line_html = line_html + doc[i].text + ' '\n",
    "    fout.write('<p>' + line_html + '</p>' + '\\n')\n",
    "fout.write('\\n')\n",
    "fout.write('<p>' + 'Статистика' + '</p>' + '\\n')\n",
    "fout.write('\\n')\n",
    "fout.write('<p>' + f'Количество слов: {quantity_words}' + '</p>' + '\\n')\n",
    "fout.write('<p>' + f'Количество терминов: {quantity_terms}' + '</p>' + '\\n')\n",
    "fout.write('<p>' + f'Доля терминов в тексте: {terms_percentage}' + '</p>' + '\\n')\n",
    "fout.write('<p>' + f'Количество уникальных терминов в тексте: {unique_terms_quantity}' + '</p>' + '\\n')\n",
    "fout.write('<p>' + f'Список уникальных терминов и их частотность: {unique_terms}' + '</p>' + '\\n')\n",
    "fout.write('</body>'+'\\n')\n",
    "fout.write('</html>'+'\\n')\n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
